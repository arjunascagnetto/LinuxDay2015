{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><h2>L'iconografia tecnologica del Grande Elefante Giallo nella Tribu' degli Apache </h2></center>\n",
    "<p><center><img src=\"http://bigdatainterviewquestions.com/wp-content/uploads/2014/10/Hadoop-Interview-Questions.jpeg\" alt=\"Drawing\" style=\"width: 200px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h4>Mai uguale, veloce e voluminosa. Se una sorgente di informazioni ha queste caratteristiche quali sono gli strumenti open source che possiamo usare per gestirla ? Facciamoci aiutare da alcuni componenti della numerosa tribù degli Apache. Hadoop, HBase, Pig, Hive, Mahout, Spark sono solo alcuni dei 30 membri della tribù, ci presenteremo e vedremo cosa hanno da dirci sul Grande Elefante Giallo</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mai uguale, veloce, voluminosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nel 2001 l'analista Doug Laney nell'articolo [\"3D data management: Controlling data volume, variety and velocity\"](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf), introduce il concetto delle 3V dei [Big Data](https://en.wikipedia.org/wiki/Big_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Volume\n",
    "- Velocity\n",
    "- Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Volume**\n",
    "\n",
    "E' decisamente la grandezza piu' nota ai media ed anche la piu' facilmente associabile alla parola \"big\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Caratterizza la quantita' di dati da processare, analizzare, ripulire, gestire, salvare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Facebook afferma che il suo database aumenta di circa [600TB al giorno](https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La API 2.0 di Facebook introdotta nel 2014 non permette l'accesso a molti elementi che la precedente API autorizzava. Ad esempio la lista degli amici dei vostri amici. Per aggirare il problema dovete scrivere una app.\n",
    "\n",
    "[Link](http://stackoverflow.com/questions/23417356/facebook-graph-api-v2-0-me-friends-returns-empty-or-only-friends-who-also-u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Velocity**\n",
    "\n",
    "Una grandezza che caratterizza la velocita' con la quale arrivano nuovi dati, da immagazzinare, gestire, processare, analizzare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Internet Live stats](http://www.internetlivestats.com/)\n",
    "\n",
    "[World Meters](http://www.worldometers.info/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Variety**\n",
    "\n",
    "Si tratta di una grandezza che va a quantificare quanto sono eterogenei i dati da immagazzinare, analizzare, processare, gestire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "E' la grandezza meno conosciuta, ma che crea piu' difficolta' nel trattarla e probabilmente e' anche la piu' interessante, o meglio la piu' densa di novita'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'eterogenita' si presenta quando si incrociano informazioni da origini diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esempio: testo, dati geografici, foto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Anche presi singolarmente i dati testo e i dati foto sono di difficile trattazione e richiedono spesso accorgimenti ad hoc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ad alcuni piace dare una veste grafica a queste grandezze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://itknowledgeexchange.techtarget.com/writing-for-business/files/2013/02/BigData.001.jpg\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quali sono allora le risposte OpenSource alla domanda che il mercato pone per il trattamenteo dei BigData ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La [Apache Software Foundation](http://www.apache.org/) **ASF** si e' preoccupata di acquisire e ridistribuire software e tecnologie che hanno avuto, hanno e avranno un posto speciale, e direi senza esagerare, un posto d'onore, nell'ecosistema dei BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[ASF BigData Category](http://projects-old.apache.org/indexes/category.html#big-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ... e fu fatto <a href=\"https://hadoop.apache.org/\"><img src=\"http://www.datameer.com/images/technology/hadoop-pic1.png\" style=\"width: 300px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tutto parte da Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Padre indiscusso del movimento Bigdata (almeno nel contesto Open Source) e' stato creato da [Doug Cutting](https://en.wikipedia.org/wiki/Doug_Cutting) e [Mike Cafarella](https://en.wikipedia.org/wiki/Mike_Cafarella) nel 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si mormora che il logo e il nome derivino dal pupazzo di peluche del figlio di Doug Cutting, appunto un elefante giallo di nome Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Che cosa e' Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Haddop e' una libreria di programmi, scritti in Java e C, che fornisce un [framework](https://it.wikipedia.org/wiki/Framework) (struttura) per l'elaborazione di grandi insiemi di dati distribuiti su un [cluster](https://it.wikipedia.org/wiki/Computer_cluster) di macchine attraverso l'uso di un paradigma di programmazione molto semplice, chiamato Map/Reduce.\n",
    "\n",
    "> The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non ha problemi di scala. Puo' andare da un cluster con pochi nodi (anche uno solo) fino a migliaia di macchine.\n",
    "\n",
    "> It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'alta affidabilita' del sistema e' ottenuta non attraverso costosi componenti hardware di controllo, ma attraverso il software Hadoop.\n",
    "\n",
    "> Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Come e' costruito un cluster Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel framework possiamo individuare 4 moduli:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Core \n",
    "- Hdfs\n",
    "- Yarn (solo in hadoop 2.0)\n",
    "- Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Di questi il ***core*** e' l'insieme delle funzioni di base, essenzialmente tutto il corredo di codice che serve per far funzionare gli altri tre moduli. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Yarn** acronimo di Yet Another Resource Negotiator. E' una evoluzione del Map/Reduce negotiator. E' il modulo che gestisce l'accesso alle risorse siano esse i files distribuiti, siano esse la ram dei nodi e/o i processori dei nodi..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "non ce ne occuperemo oggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***Hdfs*** e' il file system distribuito di Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- alcuni lo chiamano Hierarchical Distributed FileSystem \n",
    "- altri piu' semplicemente HaDoop FileSystem \n",
    "- e altri ancora Hadoop Distributed FileSystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hdfs e' famoso per le sue doti di scalabilita' e di resistenza agli errori, anche gravi come la perdita di un nodo.\n",
    "Infatti essendo un software pensato per girare su hardware lowcost, sa' geneticamente che i nodi moriranno e quindi non si agita quando cio' accade. Con una ottima gestione delle repliche sa sempre dove trovare i dati e nel caso serva prevede una loro nuova distribuzione.\n",
    "\n",
    "> HDFS is a system that knows that not all the registered components will work. Because the system is aware of this fact, it is always ready to detect any problem that might appear and start the recovery procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una particolarita' dei sistemi Hadoop-like e' che il dato e' considerato pressoche invariante. Si preferisce ricostruire il dato piuttosto che sovrascriverlo. Rivedremo questo concetto nel Map/Reduce.\n",
    "\n",
    "> HDFS is a storage that allows you to have only one writer and multiple readers. It is designed this way because of the type of data it contains. This data doesn’t change very often and that’s why it doesn’t need modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La struttura e' molto gerarchica, ma ha solo due livelli. C'e' il capo il Namenode e ci sono i lavoratori i DataNode\n",
    "\n",
    "( nella terminologia Map/reduce diventeranno il Master e i Worker )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Il datanode sa' .... dove trovare i dati . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I worker hanno .... i dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sarano i worker a fare il lavoro di analisi dei dati, ognuno per il suo pezzetto.\n",
    "\n",
    "> HDFS allows us to send the processing logic on the components where we keep the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Per approfondire:\n",
    "- \"[Hdfs Architecture Guide](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)\"\n",
    "- \"[How Hadoop Stores data](http://blog.iquestgroup.com/en/hadoop/)\"\n",
    "- \"[What are the nodes of hdfs](http://blog.iquestgroup.com/en/what-are-the-nodes-of-hdfs-hadoop-data-file-storage/)\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "... let's go to the shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Namenode WebPage](http://giove.units.it:50070/)\n",
    "\n",
    "[Datanode WebPage](http://giove.units.it:50090/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**[Map/Reduce](https://en.wikipedia.org/wiki/MapReduce)** e' un modello di calcolo nel quale ci sono due passi fondamentali che possono essere eseguiti sono in questo ordine: prima il passo di Map e poi il passo di Reduce (piu' map e piu reduce possono essere concatenati).\n",
    "\n",
    "Il Map/Reduce e' un paradigma di calcolo che rientra nella programmazione funzionale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Funziona solo con input in forma di array, lista</center> <center>cioe' dati sequenziali</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ... i valori non vengono trovati cambiando lo stato del programma, ma costruendo nuovi stati a partire dai precedenti.\n",
    ">\n",
    "-- <cite>Wikipedia<cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://blog.iquestgroup.com/en/files/2013/06/MapReduce.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Che cosa fa Map ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data una lista, Map crea una nuova lista applicando la funzione specificata ad ogni elemento.\n",
    "\n",
    "> Given a list create a new list applying a function to each element\n",
    "\n",
    "`nuovaLista = listaIniziale.map(f(x))`\n",
    "\n",
    "f : e' la funzione interna a map che verra' applicata ad ogni elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Che cosa e' Reduce ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data una lista Reduce la riduce ad un singolo valore aggregato.\n",
    "\n",
    "> Given a list iterates creating an aggregated value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vediamo un esempio molto semplice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applicazione di Map: [1, 4, 9, 16, 25] \n",
      "Applicazione di Reduce: 15\n"
     ]
    }
   ],
   "source": [
    "array = [1,2,3,4,5]\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "mappedArray = map(f,array)\n",
    "\n",
    "def g(x,y):\n",
    "    return x+y    \n",
    "\n",
    "reducedArray = reduce(g,array)\n",
    "\n",
    "\n",
    "print \"Applicazione di Map: %s \" % mappedArray\n",
    "print \"Applicazione di Reduce: %s\" % reducedArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un esempio decisamente troppo semplice, aumentiamone di poco la complessita' e vediamo cosa succede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Invece di lavorare sui valori della lista creiamo un array di coppie (chiave, valore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Accesso sequenziale\n",
    "\n",
    "Per trovare la mediana devo avere accesso a tutto il dataset, quindi Map/Reduce non puo' trovarla.\n",
    "\n",
    "In generale su Map/Reduce funzionano le operazioni almeno associative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per approfondire \n",
    "- \"[MapReduce on Hadoop](http://blog.iquestgroup.com/en/mapreduce-on-hadoop-big-data-in-action)\"\n",
    "- \"[Hadoop Fundamentals](http://hadooptutorials.co.in/tutorials/hadoop/hadoop-fundamentals.html)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cosa non e' Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Premessa.\n",
    "\n",
    "- > Nel calcolo parallelo tutti i processori devono accedere ad una memoria condivisa. La memoria condivisa può essere usata per lo scambio di informazioni tra i processori.\n",
    "\n",
    "- > Nel calcolo distribuito ogni processore ha la propria memoria privata (memoria distribuita). Le informazioni sono scambiate grazie al passaggio di messaggi tra i processori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Non e' calcolo parallelo propriamente detto poiche' non abbiamo condivisione di memoria tra i processori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I processori stanno su macchine diverse che comunicano tramite rete \n",
    "\n",
    "\"Shared-nothing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' quindi un calcolo parallelo e distribuito, poiche' di fatto i calcoli avvengono contemporaneamente su diversi processori distribuiti nel cluster. \n",
    "\n",
    "Tutto questo per dire che non si possono usare tecnologie e paradigmi di programmazione esplicitamnete studiati per il calcolo parallelo ne' per quello distribuito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ci serve o no Hadoop ?\n",
    "\n",
    "http://www.sas.com/en_us/insights/big-data/hadoop.html\n",
    "http://www.j2eebrain.com/java-J2ee-hadoop-advantages-and-disadvantages.html\n",
    "http://www.itproportal.com/2013/12/20/big-data-5-major-advantages-of-hadoop/\n",
    "https://www.thoughtworks.com/insights/blog/hadoop-or-not-hadoop\n",
    "http://www.datanami.com/2014/01/27/when_to_hadoop_and_when_not_to/\n",
    "http://www.computerworld.com/article/2501447/business-intelligence/what-s-the-big-deal-about-hadoop.html\n",
    "http://searchbusinessanalytics.techtarget.com/feature/Handling-the-hoopla-When-to-use-Hadoop-and-when-not-to\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scrivere codice in map/reduce non e' cosi' banale quando il task da svolgere diventa piu' complesso. Ne vedremo un esempio concreto nella parte dedicata a Pig. Fortunatamente esistono diverse librerie per Python che semplificano la scrittura di codice per Map/Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### MRJob pythonic way to write map-reduce programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[MRJob](https://pythonhosted.org/mrjob/index.html) e' quindi una libreria di funzioni e metodi che aiuta i programmatori a scrivere codice in Python con il paradigma Map/Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vediamo un codice di esempio - tratto dalla [guida all'installazione](https://pythonhosted.org/mrjob/guides/quickstart.html#installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_count.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"chars\"\t308\n",
      "\"lines\"\t17\n",
      "\"words\"\t31\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "python word_count.py word_count.py 1> output 2> log\n",
    "cat output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non abbiamo dovuto preoccuparci molto di istanziare le chiavi o entrare nei dettagli del raggruppamento per chiavi. In pratica noi definiamo le nostre funzioni mapper e reducer e MrJob si occupa di sistemare il mancante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inoltre con mrjob abbiamo la possibilita' di testare il codice localmente senza nemmeno la necessita' di installare un vero Hadoop. MrJob puo' occuparsi di simulare il cluster.\n",
    "\n",
    "> If you use mrjob, you’ll be able to test your code locally without installing Hadoop or run it on a cluster of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ok abbiamo visto il filesystem, abbiamo visto map/reduce, ma ai Big Data si associa inevitabilmente anche la parola database. Apache cosa puo' offrirci per archiviare, interrogare grandi tabelle ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prima bisogna interrogarsi un attimo sul concetto di Grande Tabella o piu' precisamente sulla forma del dato da trattare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I dati vengono catalogati essenzialmente con tre etichette:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Strutturati\n",
    "- Semistrutturati\n",
    "- Non strutturati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I dati **strutturati**, e' semplice, sono tutti quei dati che possono essere inseriti cosi' come stanno dentro la tabella di un database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I dati **semistrutturati** sono una categoria di mezzo, e possono essere identificati con quei dati che potrebbero essere immessi in una tabella, ma prima necessitano di qualche operazione di pulizia o di controllo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vengono considerati dati semistrutturati i file CSV, XML, JSON e ache le tabelle NoSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I dati **non strutturati** (che sono quasi l'80% dei dati mondiali) sono ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ehm, beh, i dati non strutturati sono .... hmmm non sono nessun dato che ricada nei precedenti due ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Satellite images: This includes weather data or the data that the government captures in its satellite surveillance imagery. Just think about Google Earth, and you get the picture.\n",
    "\n",
    "- Scientific data: This includes seismic imagery, atmospheric data, and high energy physics.\n",
    "\n",
    "- Photographs and video: This includes security, surveillance, and traffic video.\n",
    "\n",
    "- Radar or sonar data: This includes vehicular, meteorological, and oceanographic seismic profiles.\n",
    "\n",
    "- Text internal to your company: Think of all the text within documents, logs, survey results, and e-mails. Enterprise information actually represents a large percent of the text information in the world today.\n",
    "\n",
    "- Social media data: This data is generated from the social media platforms such as YouTube, Facebook, Twitter, LinkedIn, and Flickr.\n",
    "\n",
    "- Mobile data: This includes data such as text messages and location information.\n",
    "\n",
    "- website content: This comes from any site delivering unstructured content, like YouTube, Flickr, or Instagram.\n",
    "\n",
    "And the list goes on.\n",
    "\n",
    "<cite>https://jeremyronk.wordpress.com/2014/09/01/structured-semi-structured-and-unstructured-data/</cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Detto questo il concetto di \"big\" dei data si sta' arrichendo di sfumature... infatti se prima pensavano a grande tabella come tante rghe per tante colonne , adesso prima di tutto ci chiediamo ma di che dati stiamo parlando ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "perche' se sono strutturati allora ok righe colonne in numero spropositato, ma se sono dati semistrutturati e abbiamo migliaia di missing, usiamo ancora una tabella o forse non e' meglio usare altre strutture dati meno rigide, che non occupino indipendentemente dalla presenza o meno del dato uno spazio in memoria..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "per non paralre poi dei dati non strutturati ... esistono veramente dei \"database\" che possano gestirli ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Se ritroviamo le nostro foto su Facebook o Google+ o Istangram allora sappiamo che esistono delle strutture dati funzionali, veloci e in grado di trattare anche dati non strutturati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src =\"http://hbase.apache.org/images/hbase_logo.png\" style=\"width: 300px;\"/ >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Apache HBase™ is the Hadoop database, a distributed, scalable, big data store.\n",
    "\n",
    "Apache HBase è un database NoSQL open source basato su Hadoop e modellato su Google BigTable. HBase fornisce accesso casuale e coerenza assoluta per quantità elevate di dati non strutturati e semistrutturati in un database privo di schema, organizzato per famiglie di colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I dati sono archiviati nelle righe di una tabella e i dati di ogni riga sono raggruppati in base al tipo di colonna. HBase è un database privo di schema, poiché non è necessario definire le colonne o i tipi di dati archiviati nelle colonne prima dell'uso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Il codice open source offre scalabilità lineare, in modo da gestire petabyte di dati in migliaia di nodi. Può contare su ridondanza dei dati, elaborazione batch e altre funzionalità offerte dalle applicazioni distribuite nell'ecosistema di Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Quando dovrei usare HBase](http://hbase.apache.org/book.html#arch.overview.when)\n",
    "\n",
    "> Use Apache HBase™ when you need random, realtime read/write access to your Big Data. \n",
    "\n",
    "> This project's goal is the hosting of **very [very very] large tables** -- billions of rows X millions of columns ...\n",
    "\n",
    "> First, make sure you have enough data. If you have hundreds of millions or billions of rows, then HBase is a good candidate. **If you only have a few thousand/million rows, then using a traditional RDBMS might be a better choice** due to the fact that all of your data might wind up on a single node (or two) and the rest of the cluster may be sitting idle.\n",
    "\n",
    "> Second, make sure you can live without all the extra features that an RDBMS provides (e.g., typed columns, secondary indexes, transactions, advanced query languages, etc.) **An application built against an RDBMS cannot be \"ported\" to HBase by simply changing a JDBC driver**, for example. Consider moving from an RDBMS to HBase as a complete redesign as opposed to a port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... Hive <img src=\"https://hive.apache.org/images/hive_logo_medium.jpg\" style=\"width: 189px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> The Apache Hive ™ data warehouse software facilitates querying and managing large datasets residing in distributed storage. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL.\n",
    "\n",
    "Hive si dedica ad interrogare e gestire grandi datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ma non e' un DBMS, un Database Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem. It provides an SQL-like language called HiveQL with schema on read and transparently converts queries to map/reduce, Apache Tez and Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "... let's go to the shell !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Esempi a fine pagina.\n",
    "\n",
    "https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"https://www.elastic.co/products/hadoop\"><img src=\"https://www.elastic.co/static/img/logo-elastic.png\" style=\"width: 300px;\"></a>\n",
    "<hr>\n",
    "<a href=\"https://www.datastax.com/resources/whitepapers/hdfs-vs-cfs\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Cassandra_logo.svg/2000px-Cassandra_logo.svg.png\" style=\"width: 300px;\"></a>\n",
    "<hr>\n",
    "<a href=\"https://docs.mongodb.org/ecosystem/tools/hadoop/\"><img src=\"http://bigdatadayla.org/wp-content/uploads/2015/06/mongodb-logo-rgb.jpeg\" style=\"width: 300px;\"></a>\n",
    "<hr>\n",
    "<a href=\"http://www.dbms2.com/2010/10/12/vertica-hadoop-connector-integration/\"><img src=\"http://www.careerencore.com/Portals/79503/images/vertica-resized-600.gif\" style=\"width: 300px;\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... Pig <img src=\"https://pig.apache.org/images/pig-logo.gif\" style=\"width: 100px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pig e' prima di tutto un linguaggio per interrogare dei dataset. La sua pecularita' e' che e' una astrazione di alto livello del map/reduce. \n",
    "\n",
    "> Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tutto cio' che si scrive in codice Pig-Latin viene riscritto in Java per Hadoop Map/reduce\n",
    "\n",
    "> Pig is a high-level platform for creating MapReduce programs used with Hadoop. The language for this platform is called Pig Latin.[1] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for RDBMS systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slide con il codice in Pig Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slide con il codice in Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://mahout.apache.org/images/mahout-logo-brudman.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Libreria per il machine learning su Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scritta prevelentemente in Java, ma recentemente prevede anche Scala e ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gli algoritmi supportati sono: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Collaborative Filtering\t\t\t\t\t\n",
    "    - User-Based Collaborative Filtering\t\t\t\t\t\n",
    "    - Item-Based Collaborative Filtering\t\t\t\t\t\n",
    "    - Matrix Factorization with ALS\t\t\t\t\t\n",
    "    - Matrix Factorization with ALS on Implicit Feedback\t\t\t\t\t\n",
    "    - Weighted Matrix Factorization, SVD++\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Classification\t\t\t\t\t\n",
    "    - Logistic Regression - trained via SGD\t\t\t\t\t\n",
    "    - Naive Bayes / Complementary Naive Bayes\t\t\t\t\t\n",
    "    - Random Forest\t\t\t\t\t\n",
    "    - Hidden Markov Models\t\t\t\t\t\n",
    "    - Multilayer Perceptron\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Clustering\t\t\t\t\t\n",
    "    - Canopy Clustering\t\t\t\t\n",
    "    - k-Means Clustering\t\t\t\t\t\n",
    "    - Fuzzy k-Means\t\t\t\t\t\n",
    "    - Streaming k-Means\t\t\t\t\t\n",
    "    - Spectral Clustering\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Dimensionality Reduction \n",
    "    - Singular Value Decomposition\t\t\t\t\t\n",
    "    - Lanczos Algorithm\t\t\t\t\n",
    "    - Stochastic SVD\t\t\t\t\t\n",
    "    - PCA (via Stochastic SVD)\t\t\t\t\t\n",
    "    - QR Decomposition\t\t\t\t\t\n",
    "    - Topic Models\t\t\t\t\t\n",
    "    - Latent Dirichlet Allocation\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Miscellaneous\t\t\t\t\t\n",
    "    - RowSimilarityJob\t\t\t\t\t\n",
    "    - ConcatMatrices\t\t\t\t\t\n",
    "    - Collocations\t\t\t\t\t\n",
    "    - Sparse TF-IDF Vectors from Text\t\t\t\t\t\n",
    "    - XML Parsing\t\t\t\t\t\n",
    "    - Email Archive Parsing\t\t\t\t\t\n",
    "    - Lucene Integration\t\t\t\t\t\n",
    "    - Evolutionary Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... e fu fatta la luce <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo.png\" style=\"width: 200px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark non e' solo una astrazione del Map/Reduce, e' molto molto di piu'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- e' esso stesso un framework di calcolo distribuito\n",
    "    - standalone\n",
    "    - su Yarn\n",
    "    - su Mesos\n",
    "    - su Amazon EC2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- puo' accedere a dati archiviati su diverse \"piattaforme\"\n",
    "    - su hdfs\n",
    "    - su filesystem locale\n",
    "    - in Cassandra\n",
    "    - in HBase\n",
    "    - su Amazon EC2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ha gia' al suo interno librerie specifiche per\n",
    "    - il machine learning\n",
    "    - l'interrogazione database in SQL\n",
    "    - la gestione e l'analisi degli streaming\n",
    "    - l'analisi dei grafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' di fatto un prodotto completo e maturo, che ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... per chi non ha gia' una infrastuttura \"map/reduce\", costituisce un ottimo punto di partenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A mio personalissimo parere puo' essere Spark puo' essere considerato serenamente il punto dal quale partire per affrotnare problemi \"Big Data\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando il codice entriamo un po' piu' nel dettaglio dell'architettura di Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Carichiamo la libreria di metodi e funzioni per Python - PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inizializziamo lo SparkContext, cioe' un ambiente nel quale il codice scritto in python viene interpretato e riscritto in Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SI, ovviamente ad usare Python c'e' perdita di prestazioni, ma Python non e' famoso per la sua velocita' di esecuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"JupyterNotebookExamples\")\n",
    "#conf = SparkConf().setMaster(\"local[2]\").setAppName(\"CountingSheep\")\n",
    "#conf = SparkConf().setMaster(\"spark://spark:7077\").setAppName(\"CountingSheep\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "e facciamo delle prime operazioni con gli RDD, Resilient Distributed Datasets\n",
    "\n",
    "> The fundamental programming abstraction is called Resilient Distributed Datasets (RDDs), a logical collection of data partitioned across machines\n",
    ">\n",
    "-- <cite>Wikipedia<cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'rdd e' una struttura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dobbiamo eseguire un passo di \"reduce\", o applicare una funzione all'rdd per avere un valore o una lista ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "499500\n"
     ]
    }
   ],
   "source": [
    "print rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Carichiamo un file dal file system locale dentro lo SparkContext \"mappandolo\" come un rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The Project Gutenberg EBook of Free as in Freedom: Richard Stallman's\",\n",
       " u'Crusade for Free Software, by Sam Williams',\n",
       " u'',\n",
       " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " u're-use it under the terms of the Project Gutenberg License included',\n",
       " u'with this eBook or online at www.gutenberg.org',\n",
       " u'',\n",
       " u'** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **',\n",
       " u'**     Please follow the copyright guidelines in this file.     **']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFile = sc.textFile('data/Crusade_for_Free_Software.txt')\n",
    "print rddFile.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Facciamo un veloce word couunt sull' rddFile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The',\n",
       " u'Project',\n",
       " u'Gutenberg',\n",
       " u'EBook',\n",
       " u'of',\n",
       " u'Free',\n",
       " u'as',\n",
       " u'in',\n",
       " u'Freedom:',\n",
       " u'Richard']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddWords = rddFile.flatMap(lambda line: line.strip().split(' '))\n",
    "rddWords.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ora un passo importante nella programmazione funzionale Map/Reduce. Dobbiamo creare le coppie (key,value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel nostro caso la chiave e' la parola e il valore viene inizializzato per tutte le occorrenze di ogni singola parola a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', 1),\n",
       " (u'Project', 1),\n",
       " (u'Gutenberg', 1),\n",
       " (u'EBook', 1),\n",
       " (u'of', 1),\n",
       " (u'Free', 1),\n",
       " (u'as', 1),\n",
       " (u'in', 1),\n",
       " (u'Freedom:', 1),\n",
       " (u'Richard', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddKeyValue = rddWords.map(lambda word: (word,1))\n",
    "rddKeyValue.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ed ora facciamo un passo di reduce contando quante volte compare una certa parola, sommando il valore della chiave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 4877),\n",
       " (u'to', 2362),\n",
       " (u'of', 2213),\n",
       " (u'a', 2141),\n",
       " (u'and', 1342),\n",
       " (u'', 1156),\n",
       " (u'in', 995),\n",
       " (u'Stallman', 813),\n",
       " (u'that', 785),\n",
       " (u'was', 673),\n",
       " (u'for', 570),\n",
       " (u'as', 558),\n",
       " (u'I', 542),\n",
       " (u'it', 499),\n",
       " (u'with', 468)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCount = rddKeyValue.reduceByKey(lambda accumulator,single_value: accumulator+single_value)\n",
    "rddCount.takeOrdered(15,lambda (k,v): -v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' buona pratica da dentro un notebook, ricordarsi di chiudere lo SparkContext, per evitare problemi con altri notebook aperti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
