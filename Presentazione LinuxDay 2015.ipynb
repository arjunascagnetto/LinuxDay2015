{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>L'iconografia tecnologica del Grande Elefante Giallo nella Tribu' degli Apache </h2></center>\n",
    "<p><center><img src=\"http://bigdatainterviewquestions.com/wp-content/uploads/2014/10/Hadoop-Interview-Questions.jpeg\" alt=\"Drawing\" style=\"width: 200px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h4>Mai uguale, veloce e voluminosa. Se una sorgente di informazioni ha queste caratteristiche quali sono gli strumenti open source che possiamo usare per gestirla ? Facciamoci aiutare da alcuni componenti della numerosa tribù degli Apache. Hadoop, HBase, Pig, Hive, Mahout, Spark sono solo alcuni dei 30 membri della tribù, ci presenteremo e vedremo cosa hanno da dirci sul Grande Elefante Giallo</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mai uguale, veloce, voluminosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nel 2001 l'analista Doug Laney nell'articolo [\"3D data management: Controlling data volume, variety and velocity\"](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf), introduce il concetto delle 3V dei [Big Data](https://en.wikipedia.org/wiki/Big_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Volume\n",
    "- Velocity\n",
    "- Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Volume**\n",
    "\n",
    "E' decisamente la grandezza piu' nota ai media ed anche la piu' facilmente associabile alla parola \"big\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Caratterizza la quantita' di dati da processare, analizzare, ripulire, gestire, salvare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Facebook afferma che il suo database aumenta di circa [600TB al giorno](https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La API 2.0 di Facebook introdotta nel 2014 non permette l'accesso a molti elementi che la precedente API autorizzava. Ad esempio la lista degli amici dei vostri amici. Per aggirare il problema dovete scrivere una app e farla accettare dai vostri amici.\n",
    "\n",
    "[Link](http://stackoverflow.com/questions/23417356/facebook-graph-api-v2-0-me-friends-returns-empty-or-only-friends-who-also-u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Velocity**\n",
    "\n",
    "Una grandezza che caratterizza la velocita' con la quale arrivano nuovi dati, da immagazzinare, gestire, processare, analizzare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Internet Live stats](http://www.internetlivestats.com/)\n",
    "\n",
    "[World Meters](http://www.worldometers.info/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Variety**\n",
    "\n",
    "Si tratta di una grandezza che va a quantificare quanto sono eterogenei i dati da immagazzinare, analizzare, processare, gestire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "E' la grandezza meno conosciuta, ma che crea piu' difficolta' nel trattarla e probabilmente e' anche la piu' interessante, o meglio la piu' densa di novita'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L'eterogenita' si presenta quando si incrociano informazioni da origini diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esempio: testo, dati geografici, foto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Anche presi singolarmente i dati testo e i dati foto sono di difficile trattazione e richiedono spesso accorgimenti ad hoc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ad alcuni piace dare una veste grafica a queste grandezze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://itknowledgeexchange.techtarget.com/writing-for-business/files/2013/02/BigData.001.jpg\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quali sono allora le risposte OpenSource alla domanda che il mercato pone per il trattamenteo dei BigData ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La [Apache Software Foundation](http://www.apache.org/) **ASF** si e' preoccupata di acquisire e ridistribuire software e tecnologie che hanno avuto, hanno e avranno un posto speciale, e direi senza esagerare, un posto d'onore, nell'ecosistema dei BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[ASF BigData Category](http://projects-old.apache.org/indexes/category.html#big-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ... e fu fatto <a href=\"https://hadoop.apache.org/\"><img src=\"http://www.datameer.com/images/technology/hadoop-pic1.png\" style=\"width: 300px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tutto parte da Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Padre indiscusso del movimento Bigdata e' stato creato da [Doug Cutting](https://en.wikipedia.org/wiki/Doug_Cutting) e [Mike Cafarella](https://en.wikipedia.org/wiki/Mike_Cafarella) nel 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si mormora che il logo e il nome derivino dal pupazzo di peluche del figlio di Doug Cutting, appunto un elefante giallo di nome Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Che cosa e' Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Haddop e' una libreria di programmi, scritti in Java e C, che fornisce un [framework](https://it.wikipedia.org/wiki/Framework) (struttura) per l'elaborazione di grandi insiemi di dati distribuiti su un [cluster](https://it.wikipedia.org/wiki/Computer_cluster) di macchine attraverso l'uso di un modello di programmazione molto diretto, chiamato Map/Reduce.\n",
    "\n",
    "> The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non ha problemi di scala. Puo' andare da un cluster con pochi nodi (anche uno solo) fino a migliaia di macchine.\n",
    "\n",
    "> It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "quindi l'alta affidabilita' del sistema e' ottenuta non attraverso costosi componenti hardware di controllo, ma proprio sulla base della libreria Hadoop stessa.\n",
    "\n",
    "> Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Piu' in dettaglio come e' costruito un cluster Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel framework possiamo individuare 4 moduli:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Core \n",
    "- Hdfs\n",
    "- Yarn (solo in hadoop 2.0)\n",
    "- Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Di questi il ***core*** e' l'insieme delle funzioni di base, essenzialmente tutto il corredo di codice che serve per far funzionare gli altri tre moduli. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Yarn** acronimo di Yet Another Resource Negotiator. E' una evoluzione del Map/Reduce negotiator, il modulo che gestisce l'accesso alle risorse siano esse ad esempio i files distribuiti sul cluster hdfs siano esse la ram dei nodi e o i processori dei nodi..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " ma non ce ne occuperemo oggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***Hdfs*** e' il file system distribuito di Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- alcuni lo chiamano Hierarchical Distributed FileSystem \n",
    "- altri piu' semplicemente HaDoop FileSystem \n",
    "- e altri ancora Hadoop Distributed FileSystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per approfondire consiglio \"[How Hadoop Stores data](http://blog.iquestgroup.com/en/hadoop/)\" e \n",
    "\"[What are the nodes of hdfs](http://blog.iquestgroup.com/en/what-are-the-nodes-of-hdfs-hadoop-data-file-storage/)\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [--config confdir] COMMAND\r\n",
      "       where COMMAND is one of:\r\n",
      "  dfs                  run a filesystem command on the file systems supported in Hadoop.\r\n",
      "  namenode -format     format the DFS filesystem\r\n",
      "  secondarynamenode    run the DFS secondary namenode\r\n",
      "  namenode             run the DFS namenode\r\n",
      "  journalnode          run the DFS journalnode\r\n",
      "  zkfc                 run the ZK Failover Controller daemon\r\n",
      "  datanode             run a DFS datanode\r\n",
      "  dfsadmin             run a DFS admin client\r\n",
      "  haadmin              run a DFS HA admin client\r\n",
      "  fsck                 run a DFS filesystem checking utility\r\n",
      "  balancer             run a cluster balancing utility\r\n",
      "  jmxget               get JMX exported values from NameNode or DataNode.\r\n",
      "  mover                run a utility to move block replicas across\r\n",
      "                       storage types\r\n",
      "  oiv                  apply the offline fsimage viewer to an fsimage\r\n",
      "  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\r\n",
      "  oev                  apply the offline edits viewer to an edits file\r\n",
      "  fetchdt              fetch a delegation token from the NameNode\r\n",
      "  getconf              get config values from configuration\r\n",
      "  groups               get the groups which users belong to\r\n",
      "  snapshotDiff         diff two snapshots of a directory or diff the\r\n",
      "                       current directory contents with a snapshot\r\n",
      "  lsSnapshottableDir   list all snapshottable dirs owned by the current user\r\n",
      "\t\t\t\t\t\tUse -help to see options\r\n",
      "  portmap              run a portmap service\r\n",
      "  nfs3                 run an NFS version 3 gateway\r\n",
      "  cacheadmin           configure the HDFS cache\r\n",
      "  crypto               configure HDFS encryption zones\r\n",
      "  storagepolicies      get all the existing block storage policies\r\n",
      "  version              print the version\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/bin/hdfs --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**[Map/Reduce](https://en.wikipedia.org/wiki/MapReduce)** e' un modello di calcolo nel quale ci sono due passi fondamentali che possono essere eseguiti sono in questo ordine: prima il passo di Map e poi il passo di Reduce.\n",
    "\n",
    "Il Map/Reduce e' un paradigma di calcolo che rientra nella programmazione funzionale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Funziona solo con input in forma di array, lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ... i valori non vengono trovati cambiando lo stato del programma, ma costruendo nuovi stati a partire dai precedenti.\n",
    ">\n",
    "-- <cite>Wikipedia<cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://blog.iquestgroup.com/en/files/2013/06/MapReduce.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Che cosa fa Map ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data una lista, Map crea una nuova lista applicando la funzione specificata ad ogni elemento.\n",
    "\n",
    "> Given a list create a new list applying a function to each element\n",
    "\n",
    "`nuovaLista = listaIniziale.map(f(x))`\n",
    "\n",
    "f : e' la funzione interna a map che verra' applicata ad ogni elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Che cosa e' Reduce ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data una lista Reduce la riduce ad un singolo valore aggregato.\n",
    "\n",
    "> Given a list iterates creating an aggregated value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vediamo un esempio molto semplice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applicazione di Map: [1, 4, 9, 16, 25] \n",
      "Applicazione di Reduce: 15\n"
     ]
    }
   ],
   "source": [
    "array = [1,2,3,4,5]\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "mappedArray = map(f,array)\n",
    "\n",
    "def g(x,y):\n",
    "    return x+y    \n",
    "\n",
    "reducedArray = reduce(g,array)\n",
    "\n",
    "\n",
    "print \"Applicazione di Map: %s \" % mappedArray\n",
    "print \"Applicazione di Reduce: %s\" % reducedArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un esempio decisamente troppo semplice, ma aumentiamone di poco la complessita' e vediamo cosa succede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Invece di lavorare sui valori della lista creiamo un array di coppie (chiave, valore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- \"Map\" step: Each worker node applies the \"map()\" function to the local data, and writes the output to a temporary storage. A master node orchestrates that for redundant copies of input data, only one is processed.\n",
    "- \"Shuffle\" step: Worker nodes redistribute data based on the output keys (produced by the \"map()\" function), such that all data belonging to one key is located on the same worker node.\n",
    "- \"Reduce\" step: Worker nodes now process each group of output data, per key, in parallel.\n",
    "\n",
    "Ci sono cose che un programma scritto per il Map/Reduce non fanno bene e ci sono altre cose che fanno molto bene.\n",
    "\n",
    "Mediana non funziona bene perche' tutte le macchine devono accedere a tutto il datatset.\n",
    "Cosa fa bene ??!?!??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per approfondire \"[MapReduce on Hadoop](http://blog.iquestgroup.com/en/mapreduce-on-hadoop-big-data-in-action)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cosa non e' Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Premessa.\n",
    "\n",
    "- > Nel calcolo parallelo tutti i processori devono accedere ad una memoria condivisa. La memoria condivisa può essere usata per lo scambio di informazioni tra i processori.\n",
    "\n",
    "- > Nel calcolo distribuito ogni processore ha la propria memoria privata (memoria distribuita). Le informazioni sono scambiate grazie al passaggio di messaggi tra i processori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Non e' calcolo parallelo propriamente detto poiche' non abbiamo condivisione di memoria tra i processori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I processori stanno su macchine diverse che comunicano tramite rete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' quindi un calcolo parallelo e distribuito, poiche' di fatto i calcoli avvengono contemporaneamente su diversi processori distribuiti nel cluster. \n",
    "\n",
    "Tutto questo per dire che non si possono usare tecnologie e paradigmi di programmazione esplicitamnete studiati per il calcolo parallelo ne' per quello distribuito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spiegazione dell' hdfs con esempi su cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spiegazione del Map/Reduce usando il word count con codice cineca-like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### MRJob pythonic way to write map-reduce programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[MRJob](https://pythonhosted.org/mrjob/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src =\"http://hbase.apache.org/images/hbase_logo.png\" style=\"width: 300px;\"/ >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Database distribuito su HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Cassandra_logo.svg/220px-Cassandra_logo.svg.png\" style=\"width: 200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Database distribuito fault-tollerant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table><tr><td><img src=\"https://pig.apache.org/images/pig-logo.gif\" style=\"width: 100px;\" /></td><td><img src=\"https://hive.apache.org/images/hive_logo_medium.jpg\" style=\"width: 100px;\" /></td></tr><tr><td><center>Pig</center></td> <td><center>Hive</center></td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Pig is a high-level platform for creating MapReduce programs used with Hadoop. The language for this platform is called Pig Latin.[1] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for RDBMS systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slide con il codice in Pig Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slide con il codice in Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> The Apache Hive ™ data warehouse software facilitates querying and managing large datasets residing in distributed storage. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem. It provides an SQL-like language called HiveQL[6] with schema on read and transparently converts queries to map/reduce, Apache Tez[7] and Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://mahout.apache.org/images/mahout-logo-brudman.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Libreria per il machine learning su Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scritta prevelentemente in Java, ma recentemente prevede anche Scala e ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gli algoritmi supportati sono: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Collaborative Filtering\t\t\t\t\t\n",
    "    - User-Based Collaborative Filtering\t\t\t\t\t\n",
    "    - Item-Based Collaborative Filtering\t\t\t\t\t\n",
    "    - Matrix Factorization with ALS\t\t\t\t\t\n",
    "    - Matrix Factorization with ALS on Implicit Feedback\t\t\t\t\t\n",
    "    - Weighted Matrix Factorization, SVD++\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Classification\t\t\t\t\t\n",
    "    - Logistic Regression - trained via SGD\t\t\t\t\t\n",
    "    - Naive Bayes / Complementary Naive Bayes\t\t\t\t\t\n",
    "    - Random Forest\t\t\t\t\t\n",
    "    - Hidden Markov Models\t\t\t\t\t\n",
    "    - Multilayer Perceptron\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Clustering\t\t\t\t\t\n",
    "    - Canopy Clustering\t\t\t\t\n",
    "    - k-Means Clustering\t\t\t\t\t\n",
    "    - Fuzzy k-Means\t\t\t\t\t\n",
    "    - Streaming k-Means\t\t\t\t\t\n",
    "    - Spectral Clustering\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Dimensionality Reduction \n",
    "    - Singular Value Decomposition\t\t\t\t\t\n",
    "    - Lanczos Algorithm\t\t\t\t\n",
    "    - Stochastic SVD\t\t\t\t\t\n",
    "    - PCA (via Stochastic SVD)\t\t\t\t\t\n",
    "    - QR Decomposition\t\t\t\t\t\n",
    "    - Topic Models\t\t\t\t\t\n",
    "    - Latent Dirichlet Allocation\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Miscellaneous\t\t\t\t\t\n",
    "    - RowSimilarityJob\t\t\t\t\t\n",
    "    - ConcatMatrices\t\t\t\t\t\n",
    "    - Collocations\t\t\t\t\t\n",
    "    - Sparse TF-IDF Vectors from Text\t\t\t\t\t\n",
    "    - XML Parsing\t\t\t\t\t\n",
    "    - Email Archive Parsing\t\t\t\t\t\n",
    "    - Lucene Integration\t\t\t\t\t\n",
    "    - Evolutionary Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... e fu fatta la luce <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo.png\" style=\"width: 200px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark non e' solo una astrazione del Map/Reduce, e' molto molto di piu'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e' esso stesso un framework di calcolo distribuito\n",
    "    - standalone\n",
    "    - su Yarn\n",
    "    - su Mesos\n",
    "    - su Amazon EC2\n",
    "    \n",
    "- puo' accedere a dati archiviati su diverse \"piattaforme\"\n",
    "    - su hdfs\n",
    "    - su filesystem locale\n",
    "    - in Cassandra\n",
    "    - in HBase\n",
    "    - su Amazon EC2\n",
    "    \n",
    "- ha gia' al suo interno librerie specifiche per\n",
    "    - il machine learning\n",
    "    - l'interrogazione database in SQL\n",
    "    - la gestione e l'analisi degli streaming\n",
    "    - l'analisi dei grafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' di fatto un prodotto completo e maturo, che ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... per chi non ha gia' una infrastuttura \"map/reduce\", costituisce un ottimo punto di partenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> E' il punto di partenza \n",
    "> \n",
    "-- <cite>Citazione da me stesso</cite> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Esempi di codice Spark con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Carichiamo la libreria di metodi e funzioni per Python - PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inizializziamo lo SparkContext, cioe' un ambiente nel quale il codice scritto in python viene interpretato e riscritto in Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SI, ovviamente ad usare Python c'e' perdita di prestazioni, ma Python non e' famoso per la sua velocita' di esecuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"JupyterNotebookExamples\")\n",
    "#conf = SparkConf().setMaster(\"local[2]\").setAppName(\"CountingSheep\")\n",
    "#conf = SparkConf().setMaster(\"spark://spark:7077\").setAppName(\"CountingSheep\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "e facciamo delle prime operazioni con gli RDD, Resilient Distributed Datasets\n",
    "\n",
    "> The fundamental programming abstraction is called Resilient Distributed Datasets (RDDs), a logical collection of data partitioned across machines\n",
    ">\n",
    "-- <cite>Wikipedia<cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "499500\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,1000))\n",
    "print rdd.count()\n",
    "print rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Carichiamo un file dal file system locale dentro lo SparkContext \"mappandolo\" come un rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The Project Gutenberg EBook of Free as in Freedom: Richard Stallman's\",\n",
       " u'Crusade for Free Software, by Sam Williams',\n",
       " u'',\n",
       " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " u're-use it under the terms of the Project Gutenberg License included',\n",
       " u'with this eBook or online at www.gutenberg.org',\n",
       " u'',\n",
       " u'** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **',\n",
       " u'**     Please follow the copyright guidelines in this file.     **']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFile = sc.textFile('data/Crusade_for_Free_Software.txt')\n",
    "rddFile.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il WordCount sta' a Spark/Hadoop come l' \"Hello World!\" sta' ai linguaggi di programmazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Facciamo un veloce word couunt sull' rddFile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The',\n",
       " u'Project',\n",
       " u'Gutenberg',\n",
       " u'EBook',\n",
       " u'of',\n",
       " u'Free',\n",
       " u'as',\n",
       " u'in',\n",
       " u'Freedom:',\n",
       " u'Richard']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddWords = rddFile.flatMap(lambda line: line.strip().split(' '))\n",
    "rddWords.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ora un passo importante nella programmazione funzionale Map/Reduce. Dobbiamo creare le coppie (key,value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel nostro caso la chiave e' la parola e il valore viene inizializzato per tutte le occorrenze di ogni singola parola a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', 1),\n",
       " (u'Project', 1),\n",
       " (u'Gutenberg', 1),\n",
       " (u'EBook', 1),\n",
       " (u'of', 1),\n",
       " (u'Free', 1),\n",
       " (u'as', 1),\n",
       " (u'in', 1),\n",
       " (u'Freedom:', 1),\n",
       " (u'Richard', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddKeyValue = rddWords.map(lambda word: (word,1))\n",
    "rddKeyValue.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ed ora facciamo un passo di reduce contando quante volte compare una certa parola, sommando il valore della chiave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 4877),\n",
       " (u'to', 2362),\n",
       " (u'of', 2213),\n",
       " (u'a', 2141),\n",
       " (u'and', 1342),\n",
       " (u'', 1156),\n",
       " (u'in', 995),\n",
       " (u'Stallman', 813),\n",
       " (u'that', 785),\n",
       " (u'was', 673),\n",
       " (u'for', 570),\n",
       " (u'as', 558),\n",
       " (u'I', 542),\n",
       " (u'it', 499),\n",
       " (u'with', 468)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCount = rddKeyValue.reduceByKey(lambda accumulator,single_value: accumulator+single_value)\n",
    "rddCount.takeOrdered(15,lambda (k,v): -v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' buona pratica da dentro un notebook, ricordarsi di chiudere lo SparkContext, per evitare problemi con altri notebook aperti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
