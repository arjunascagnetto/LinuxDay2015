{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>L'iconografia tecnologica del Grande Elefante Giallo nella Tribu' degli Apache </h2></center>\n",
    "<p><center><img src=\"http://bigdatainterviewquestions.com/wp-content/uploads/2014/10/Hadoop-Interview-Questions.jpeg\" alt=\"Drawing\" style=\"width: 200px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h4>Mai uguale, veloce e voluminosa. Se una sorgente di informazioni ha queste caratteristiche quali sono gli strumenti open source che possiamo usare per gestirla ? Facciamoci aiutare da alcuni componenti della numerosa tribù degli Apache. Hadoop, HBase, Pig, Hive, Mahout, Spark sono solo alcuni dei 30 membri della tribù, ci presenteremo e vedremo cosa hanno da dirci sul Grande Elefante Giallo</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mai uguale, veloce, voluminosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nel 2001 l'analista Doug Laney nell'articolo [\"3D data management: Controlling data volume, variety and velocity\"](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf), introduce il concetto delle 3V dei [Big Data](https://en.wikipedia.org/wiki/Big_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Volume\n",
    "- Velocity\n",
    "- Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Volume**\n",
    "\n",
    "E' decisamente la grandezza piu' nota ai media ed anche la piu' facilmente associabile alla parola \"big\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Caratterizza la quantita' di dati da processare, analizzare, ripulire, gestire, salvare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Facebook afferma che il suo database aumenta di circa [600TB al giorno](https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La API 2.0 di Facebook introdotta nel 2014 non permette l'accesso a molti elementi che la precedente API autorizzava. Ad esempio la lista degli amici dei vostri amici. Per aggirare il problema dovete scrivere una app e farla accettare dai vostri amici.\n",
    "\n",
    "[Link](http://stackoverflow.com/questions/23417356/facebook-graph-api-v2-0-me-friends-returns-empty-or-only-friends-who-also-u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Velocity**\n",
    "\n",
    "Una grandezza che caratterizza la velocita' con la quale arrivano nuovi dati, da immagazzinare, gestire, processare, analizzare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Internet Live stats](http://www.internetlivestats.com/)\n",
    "\n",
    "[World Meters](http://www.worldometers.info/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Variety**\n",
    "\n",
    "Si tratta di una grandezza che va a quantificare quanto sono eterogenei i dati da immagazzinare, analizzare, processare, gestire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "E' la grandezza meno conosciuta, ma che crea piu' difficolta' nel trattarla e probabilmente e' anche la piu' interessante, o meglio la piu' densa di novita'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'eterogenita' si presenta quando si incrociano informazioni da origini diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esempio: testo, dati geografici, foto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Anche presi singolarmente i dati testo e i dati foto sono di difficile trattazione e richiedono spesso accorgimenti ad hoc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ad alcuni piace dare una veste grafica a queste grandezze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://itknowledgeexchange.techtarget.com/writing-for-business/files/2013/02/BigData.001.jpg\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quali sono allora le risposte OpenSource alla domanda che il mercato pone per il trattamenteo dei BigData ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La [Apache Software Foundation](http://www.apache.org/) **ASF** si e' preoccupata di acquisire e ridistribuire software e tecnologie che hanno avuto, hanno e avranno un posto speciale, e direi senza esagerare, un posto d'onore, nell'ecosistema dei BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[ASF BigData Category](http://projects-old.apache.org/indexes/category.html#big-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ... e fu fatto <a href=\"https://hadoop.apache.org/\"><img src=\"http://www.datameer.com/images/technology/hadoop-pic1.png\" style=\"width: 300px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tutto parte da Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Padre indiscusso del movimento Bigdata e' stato creato da [Doug Cutting](https://en.wikipedia.org/wiki/Doug_Cutting) e [Mike Cafarella](https://en.wikipedia.org/wiki/Mike_Cafarella) nel 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si mormora che il logo e il nome derivino dal pupazzo di peluche del figlio di Doug Cutting, appunto un elefante giallo di nome Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Che cosa e' Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Haddop e' una libreria di programmi, scritti in Java e C, che fornisce un [framework](https://it.wikipedia.org/wiki/Framework) (struttura) per l'elaborazione di grandi insiemi di dati distribuiti su un [cluster](https://it.wikipedia.org/wiki/Computer_cluster) di macchine attraverso l'uso di un modello di programmazione molto diretto, chiamato Map/Reduce.\n",
    "\n",
    "> The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non ha problemi di scala. Puo' andare da un cluster con pochi nodi (anche uno solo) fino a migliaia di macchine.\n",
    "\n",
    "> It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "quindi l'alta affidabilita' del sistema e' ottenuta non attraverso costosi componenti hardware di controllo, ma proprio sulla base della libreria Hadoop stessa.\n",
    "\n",
    "> Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Piu' in dettaglio come e' costruito un cluster Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel framework possiamo individuare 4 moduli:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Core \n",
    "- Hdfs\n",
    "- Yarn (solo in hadoop 2.0)\n",
    "- Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Di questi il ***core*** e' l'insieme delle funzioni di base, essenzialmente tutto il corredo di codice che serve per far funzionare gli altri tre moduli. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Yarn** acronimo di Yet Another Resource Negotiator. E' una evoluzione del Map/Reduce negotiator, il modulo che gestisce l'accesso alle risorse siano esse ad esempio i files distribuiti sul cluster hdfs siano esse la ram dei nodi e o i processori dei nodi..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " ma non ce ne occuperemo oggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***Hdfs*** e' il file system distribuito di Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- alcuni lo chiamano Hierarchical Distributed FileSystem \n",
    "- altri piu' semplicemente HaDoop FileSystem \n",
    "- e altri ancora Hadoop Distributed FileSystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per approfondire \"[How Hadoop Stores data](http://blog.iquestgroup.com/en/hadoop/)\" e \n",
    "\"[What are the nodes of hdfs](http://blog.iquestgroup.com/en/what-are-the-nodes-of-hdfs-hadoop-data-file-storage/)\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Namenode WebPage](http://giove.units.it:50070/)\n",
    "\n",
    "[Datanode WebPage](http://giove.units.it:50090/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/bin/hdfs dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2015-10-22 13:47 /data\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**[Map/Reduce](https://en.wikipedia.org/wiki/MapReduce)** e' un modello di calcolo nel quale ci sono due passi fondamentali che possono essere eseguiti sono in questo ordine: prima il passo di Map e poi il passo di Reduce.\n",
    "\n",
    "Il Map/Reduce e' un paradigma di calcolo che rientra nella programmazione funzionale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Funziona solo con input in forma di array, lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ... i valori non vengono trovati cambiando lo stato del programma, ma costruendo nuovi stati a partire dai precedenti.\n",
    ">\n",
    "-- <cite>Wikipedia<cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://blog.iquestgroup.com/en/files/2013/06/MapReduce.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Che cosa fa Map ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data una lista, Map crea una nuova lista applicando la funzione specificata ad ogni elemento.\n",
    "\n",
    "> Given a list create a new list applying a function to each element\n",
    "\n",
    "`nuovaLista = listaIniziale.map(f(x))`\n",
    "\n",
    "f : e' la funzione interna a map che verra' applicata ad ogni elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Che cosa e' Reduce ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data una lista Reduce la riduce ad un singolo valore aggregato.\n",
    "\n",
    "> Given a list iterates creating an aggregated value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vediamo un esempio molto semplice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applicazione di Map: [1, 4, 9, 16, 25] \n",
      "Applicazione di Reduce: 15\n"
     ]
    }
   ],
   "source": [
    "array = [1,2,3,4,5]\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "mappedArray = map(f,array)\n",
    "\n",
    "def g(x,y):\n",
    "    return x+y    \n",
    "\n",
    "reducedArray = reduce(g,array)\n",
    "\n",
    "\n",
    "print \"Applicazione di Map: %s \" % mappedArray\n",
    "print \"Applicazione di Reduce: %s\" % reducedArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un esempio decisamente troppo semplice, aumentiamone di poco la complessita' e vediamo cosa succede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Invece di lavorare sui valori della lista creiamo un array di coppie (chiave, valore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Accesso sequenziale\n",
    "\n",
    "Per trovare la mediana devo avere accesso a tutto il dataset, quindi Map/Reduce non puo' trovarla.\n",
    "\n",
    "In generale su Map/Reduce funzionano le operazioni almeno associative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per approfondire \n",
    "- \"[MapReduce on Hadoop](http://blog.iquestgroup.com/en/mapreduce-on-hadoop-big-data-in-action)\"\n",
    "- \"[Hadoop Fundamentals](http://hadooptutorials.co.in/tutorials/hadoop/hadoop-fundamentals.html)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cosa non e' Hadoop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Premessa.\n",
    "\n",
    "- > Nel calcolo parallelo tutti i processori devono accedere ad una memoria condivisa. La memoria condivisa può essere usata per lo scambio di informazioni tra i processori.\n",
    "\n",
    "- > Nel calcolo distribuito ogni processore ha la propria memoria privata (memoria distribuita). Le informazioni sono scambiate grazie al passaggio di messaggi tra i processori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Non e' calcolo parallelo propriamente detto poiche' non abbiamo condivisione di memoria tra i processori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I processori stanno su macchine diverse che comunicano tramite rete \n",
    "\n",
    "\"Shared-nothing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' quindi un calcolo parallelo e distribuito, poiche' di fatto i calcoli avvengono contemporaneamente su diversi processori distribuiti nel cluster. \n",
    "\n",
    "Tutto questo per dire che non si possono usare tecnologie e paradigmi di programmazione esplicitamnete studiati per il calcolo parallelo ne' per quello distribuito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci serve o no Hadoop ?\n",
    "\n",
    "http://www.sas.com/en_us/insights/big-data/hadoop.html\n",
    "http://www.j2eebrain.com/java-J2ee-hadoop-advantages-and-disadvantages.html\n",
    "http://www.itproportal.com/2013/12/20/big-data-5-major-advantages-of-hadoop/\n",
    "https://www.thoughtworks.com/insights/blog/hadoop-or-not-hadoop\n",
    "http://www.datanami.com/2014/01/27/when_to_hadoop_and_when_not_to/\n",
    "http://www.computerworld.com/article/2501447/business-intelligence/what-s-the-big-deal-about-hadoop.html\n",
    "http://searchbusinessanalytics.techtarget.com/feature/Handling-the-hoopla-When-to-use-Hadoop-and-when-not-to\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scrivere codice in map/reduce non e' cosi' banale quando il task da svolgere diventa piu' complesso. Ne vedremo un esempio concreto nella parte dedicata a Pig. Fortunatamente esistono diverse librerie per Python che semplificano la scrittura di codice per Map/Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### MRJob pythonic way to write map-reduce programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[MRJob](https://pythonhosted.org/mrjob/index.html) e' quindi una libreria di funzioni e metodi che aiuta i programmatori a scrivere codice in Python con il paradigma Map/Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo un codice di esempio - tratto dalla [guida all'installazione](https://pythonhosted.org/mrjob/guides/quickstart.html#installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_count.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"chars\"\t308\n",
      "\"lines\"\t17\n",
      "\"words\"\t31\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "python word_count.py word_count.py 1> output 2> log\n",
    "cat output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non abbiamo dovuto preoccuparci molto di istanziare le chiavi o entrare nei dettagli del raggruppamento per chiavi. In pratica noi definiamo le nostre funzioni mapper e reducer e MrJob si occupa di sistemare il mancante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inoltre con mrjob abbiamo la possibilita' di testare il codice localmente senza nemmeno la necessita' di installare un vero Hadoop. MrJob puo' occuparsi di simulare il cluster.\n",
    "\n",
    "> If you use mrjob, you’ll be able to test your code locally without installing Hadoop or run it on a cluster of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok abbiamo visto il filesystem, abbiamo visto map/reduce, ma ai Big Data si associa inevitabilmente anche la parola database. Apache cosa puo' offrirci per archiviare, interrogare grandi tabelle ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima bisogna interrogarsi un attimo sul concetto di Grande Tabella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src =\"http://hbase.apache.org/images/hbase_logo.png\" style=\"width: 300px;\"/ >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Apache HBase™ is the Hadoop database, a distributed, scalable, big data store.\n",
    "\n",
    "Apache HBase è un database NoSQL open source basato su Hadoop e modellato su Google BigTable. HBase fornisce accesso casuale e coerenza assoluta per quantità elevate di dati non strutturati e semistrutturati in un database privo di schema, organizzato per famiglie di colonne.\n",
    "\n",
    "I dati sono archiviati nelle righe di una tabella e i dati di ogni riga sono raggruppati in base al tipo di colonna. HBase è un database privo di schema, poiché non è necessario definire le colonne o i tipi di dati archiviati nelle colonne prima dell'uso. Il codice open source offre scalabilità lineare, in modo da gestire petabyte di dati in migliaia di nodi. Può contare su ridondanza dei dati, elaborazione batch e altre funzionalità offerte dalle applicazioni distribuite nell'ecosistema di Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Quando dovrei usare HBase](http://hbase.apache.org/book.html#arch.overview.when)\n",
    "\n",
    "> Use Apache HBase™ when you need random, realtime read/write access to your Big Data. \n",
    "\n",
    "> This project's goal is the hosting of very large tables -- billions of rows X millions of columns ...\n",
    "\n",
    "> First, make sure you have enough data. If you have hundreds of millions or billions of rows, then HBase is a good candidate. If you only have a few thousand/million rows, then using a traditional RDBMS might be a better choice due to the fact that all of your data might wind up on a single node (or two) and the rest of the cluster may be sitting idle.\n",
    "\n",
    "> Second, make sure you can live without all the extra features that an RDBMS provides (e.g., typed columns, secondary indexes, transactions, advanced query languages, etc.) An application built against an RDBMS cannot be \"ported\" to HBase by simply changing a JDBC driver, for example. Consider moving from an RDBMS to HBase as a complete redesign as opposed to a port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... Hive <img src=\"https://hive.apache.org/images/hive_logo_medium.jpg\" style=\"width: 189px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> The Apache Hive ™ data warehouse software facilitates querying and managing large datasets residing in distributed storage. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL.\n",
    "\n",
    "Hive si dedica ad interrogare e gestire grandi datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ma non DBMS, un Database Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem. It provides an SQL-like language called HiveQL with schema on read and transparently converts queries to map/reduce, Apache Tez and Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Vai a vedere gli esempi alla fine della pagina.\n",
    "\n",
    "https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... Pig <img src=\"https://pig.apache.org/images/pig-logo.gif\" style=\"width: 100px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Pig is a high-level platform for creating MapReduce programs used with Hadoop. The language for this platform is called Pig Latin.[1] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for RDBMS systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slide con il codice in Pig Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slide con il codice in Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://mahout.apache.org/images/mahout-logo-brudman.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Libreria per il machine learning su Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scritta prevelentemente in Java, ma recentemente prevede anche Scala e ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gli algoritmi supportati sono: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Collaborative Filtering\t\t\t\t\t\n",
    "    - User-Based Collaborative Filtering\t\t\t\t\t\n",
    "    - Item-Based Collaborative Filtering\t\t\t\t\t\n",
    "    - Matrix Factorization with ALS\t\t\t\t\t\n",
    "    - Matrix Factorization with ALS on Implicit Feedback\t\t\t\t\t\n",
    "    - Weighted Matrix Factorization, SVD++\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Classification\t\t\t\t\t\n",
    "    - Logistic Regression - trained via SGD\t\t\t\t\t\n",
    "    - Naive Bayes / Complementary Naive Bayes\t\t\t\t\t\n",
    "    - Random Forest\t\t\t\t\t\n",
    "    - Hidden Markov Models\t\t\t\t\t\n",
    "    - Multilayer Perceptron\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Clustering\t\t\t\t\t\n",
    "    - Canopy Clustering\t\t\t\t\n",
    "    - k-Means Clustering\t\t\t\t\t\n",
    "    - Fuzzy k-Means\t\t\t\t\t\n",
    "    - Streaming k-Means\t\t\t\t\t\n",
    "    - Spectral Clustering\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Dimensionality Reduction \n",
    "    - Singular Value Decomposition\t\t\t\t\t\n",
    "    - Lanczos Algorithm\t\t\t\t\n",
    "    - Stochastic SVD\t\t\t\t\t\n",
    "    - PCA (via Stochastic SVD)\t\t\t\t\t\n",
    "    - QR Decomposition\t\t\t\t\t\n",
    "    - Topic Models\t\t\t\t\t\n",
    "    - Latent Dirichlet Allocation\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Miscellaneous\t\t\t\t\t\n",
    "    - RowSimilarityJob\t\t\t\t\t\n",
    "    - ConcatMatrices\t\t\t\t\t\n",
    "    - Collocations\t\t\t\t\t\n",
    "    - Sparse TF-IDF Vectors from Text\t\t\t\t\t\n",
    "    - XML Parsing\t\t\t\t\t\n",
    "    - Email Archive Parsing\t\t\t\t\t\n",
    "    - Lucene Integration\t\t\t\t\t\n",
    "    - Evolutionary Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ... e fu fatta la luce <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo.png\" style=\"width: 200px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark non e' solo una astrazione del Map/Reduce, e' molto molto di piu'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e' esso stesso un framework di calcolo distribuito\n",
    "    - standalone\n",
    "    - su Yarn\n",
    "    - su Mesos\n",
    "    - su Amazon EC2\n",
    "    \n",
    "- puo' accedere a dati archiviati su diverse \"piattaforme\"\n",
    "    - su hdfs\n",
    "    - su filesystem locale\n",
    "    - in Cassandra\n",
    "    - in HBase\n",
    "    - su Amazon EC2\n",
    "    \n",
    "- ha gia' al suo interno librerie specifiche per\n",
    "    - il machine learning\n",
    "    - l'interrogazione database in SQL\n",
    "    - la gestione e l'analisi degli streaming\n",
    "    - l'analisi dei grafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' di fatto un prodotto completo e maturo, che ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... per chi non ha gia' una infrastuttura \"map/reduce\", costituisce un ottimo punto di partenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> E' il punto di partenza \n",
    "> \n",
    "-- <cite>Citazione da me stesso</cite> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Esempi di codice Spark con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Carichiamo la libreria di metodi e funzioni per Python - PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inizializziamo lo SparkContext, cioe' un ambiente nel quale il codice scritto in python viene interpretato e riscritto in Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SI, ovviamente ad usare Python c'e' perdita di prestazioni, ma Python non e' famoso per la sua velocita' di esecuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"JupyterNotebookExamples\")\n",
    "#conf = SparkConf().setMaster(\"local[2]\").setAppName(\"CountingSheep\")\n",
    "#conf = SparkConf().setMaster(\"spark://spark:7077\").setAppName(\"CountingSheep\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "e facciamo delle prime operazioni con gli RDD, Resilient Distributed Datasets\n",
    "\n",
    "> The fundamental programming abstraction is called Resilient Distributed Datasets (RDDs), a logical collection of data partitioned across machines\n",
    ">\n",
    "-- <cite>Wikipedia<cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "499500\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,1000))\n",
    "print rdd.count()\n",
    "print rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Carichiamo un file dal file system locale dentro lo SparkContext \"mappandolo\" come un rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The Project Gutenberg EBook of Free as in Freedom: Richard Stallman's\",\n",
       " u'Crusade for Free Software, by Sam Williams',\n",
       " u'',\n",
       " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " u're-use it under the terms of the Project Gutenberg License included',\n",
       " u'with this eBook or online at www.gutenberg.org',\n",
       " u'',\n",
       " u'** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **',\n",
       " u'**     Please follow the copyright guidelines in this file.     **']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFile = sc.textFile('data/Crusade_for_Free_Software.txt')\n",
    "rddFile.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il WordCount sta' a Spark/Hadoop come l' \"Hello World!\" sta' ai linguaggi di programmazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Facciamo un veloce word couunt sull' rddFile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The',\n",
       " u'Project',\n",
       " u'Gutenberg',\n",
       " u'EBook',\n",
       " u'of',\n",
       " u'Free',\n",
       " u'as',\n",
       " u'in',\n",
       " u'Freedom:',\n",
       " u'Richard']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddWords = rddFile.flatMap(lambda line: line.strip().split(' '))\n",
    "rddWords.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ora un passo importante nella programmazione funzionale Map/Reduce. Dobbiamo creare le coppie (key,value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel nostro caso la chiave e' la parola e il valore viene inizializzato per tutte le occorrenze di ogni singola parola a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', 1),\n",
       " (u'Project', 1),\n",
       " (u'Gutenberg', 1),\n",
       " (u'EBook', 1),\n",
       " (u'of', 1),\n",
       " (u'Free', 1),\n",
       " (u'as', 1),\n",
       " (u'in', 1),\n",
       " (u'Freedom:', 1),\n",
       " (u'Richard', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddKeyValue = rddWords.map(lambda word: (word,1))\n",
    "rddKeyValue.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ed ora facciamo un passo di reduce contando quante volte compare una certa parola, sommando il valore della chiave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 4877),\n",
       " (u'to', 2362),\n",
       " (u'of', 2213),\n",
       " (u'a', 2141),\n",
       " (u'and', 1342),\n",
       " (u'', 1156),\n",
       " (u'in', 995),\n",
       " (u'Stallman', 813),\n",
       " (u'that', 785),\n",
       " (u'was', 673),\n",
       " (u'for', 570),\n",
       " (u'as', 558),\n",
       " (u'I', 542),\n",
       " (u'it', 499),\n",
       " (u'with', 468)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCount = rddKeyValue.reduceByKey(lambda accumulator,single_value: accumulator+single_value)\n",
    "rddCount.takeOrdered(15,lambda (k,v): -v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E' buona pratica da dentro un notebook, ricordarsi di chiudere lo SparkContext, per evitare problemi con altri notebook aperti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
